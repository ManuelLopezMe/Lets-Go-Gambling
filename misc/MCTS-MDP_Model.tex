\documentclass[12pt,a4paper]{cibb}
\usepackage{enumitem}
\usepackage{float}

\usepackage{subfigure,graphicx}
\usepackage{amsmath,amsfonts,latexsym,amssymb,euscript,xr}
\usepackage{booktabs}
\usepackage[nodayofweek]{datetime}
\usepackage{hyperref}
\usepackage[table]{xcolor}
\usepackage{color,colortbl,tabularx}
\usepackage[english]{babel}
\usepackage[protrusion=true,expansion=true]{microtype}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{pifont}

\definecolor{LightBlue}{rgb}{0.88,0.9,0.9}

\def\red{\color{red}}
\def\black{\color{black}}
\def\blue{\color{blue}}
\def\magenta{\color{magenta}}

\title{\Large $\ $\\ \bf Finding the Optimal Blackjack Strategy Using Markov Decision Process Monte Carlo}

\author{\large Manuel Lopez-Mejia}
\address{\footnotesize 
College of Engineering, North Carolina State University, Raleigh, USA, manuel.lopezme05@gmail.com, 92855
}

\abstract{\small Blackjack, Monte Carlo Tree Search, Markov Decision Process, Markov Chains, Dynamic Programming. \normalsize
\\[17pt]
{\bf Abstract.} This paper investigates the application of Bayesian probability to predict subsequent card draws in Blackjack. Leveraging these probabilistic predictions, we calculate the expected value for key player decisions (Hit, Stand, Double Down, Split) to identify strategies that maximize expected returns. The player's sequential decision-making problem is modeled as a Markov Decision Process (MDP). A Monte Carlo Tree Search (MCTS) algorithm is implemented to estimate the optimal action-value functions within this MDP framework by averaging returns over numerous simulated game episodes. This approach aims to derive a near-optimal playing strategy based on probabilistic inference and reinforcement learning techniques.}

\begin{document}
\thispagestyle{myheadings}
\pagestyle{myheadings}

\section{Introduction}
\label{sec:SCIENTIFIC-BACKGROUND}

Blackjack is a popular card game where you try to reach a hand as close to 21 as possible without going over (busting). If you are able to have a higher hand than the dealer's hand, you win. Cards are ranked with values from 1 to 11, with Ace being worth 1 or 11 (whichever is optimal) and face cards (Kings, Queens, and Jacks) being valued at 10.

Players and the dealer are initially dealt a pair of cards which are shown to everyone except for one of the dealer's cards. Players move first, choosing to either stand, hit (draw one more card), double down (double one's wager by drawing one card and then stand), and if dealt a hand with a pair of the exact same cards, split where you can split your current hand into two new hands and wager the same amount on each. After a player stands, the dealer's hidden card is shown and he must hit until he reaches at least 17. For every round of Blackjack played, cards are drawn without replacement until they're reshuffled, typically once 15 to 50 percent of the game deck has been drawn.

Given these rules, particularly the sequential nature of decisions, the dealer's fixed strategy, and the changing composition of the deck due to drawing without replacement, determining the optimal action at any given point presents a complex challenge. The player must constantly assess the risk of busting against the potential reward of improving their hand relative to the dealer's likely outcome, making Blackjack a fascinating domain for applying strategic analysis and Bayesian reasoning. 

Contemporary computing experiments simulate games that utilize infinite decks (thus ignoring card composition) \cite{robprattsas} \cite{connie-trojan} \cite{aman-khurna} and are thus solved are fairly tractable games that can be solved using traditional linear programming methods. Other limitations of these experiments are that they only handle hitting and standing while ignoring doubling down and splitting (the latter which requires recursive programming) and are not very applicable to most contemporary casino versions of the game.

Khurna (2020) uses an MDP approach following Sutton and Barto (2015) but, in addition to the limitations above, uses a Q-learning method for reinforcement learning rather than Monte Carlo as prescribed by Sutton and Barto. Q-learning requires an agent to explore every possible game state which may be reasonable in a tractable environment. But in an environment where we want our agent to keep a measure of the shoe composition (e.g., "the true count"), accurately learning Q-values across this vast, dynamic space is extremely challenging and data-intensive. On the other hand, MCTS does not need to explicitly learn values for every possible state. Instead, it performs simulations from the current state (which includes the current shoe composition). It builds a search tree on the fly, exploring relevant future states based on the current probabilities derived from the shoe. This makes it much more scalable to large state spaces driven by factors such as card counting.

\section{Background}
\label{sec:Theory}

\subsection{{Bayesian Probability}}

Since the game is played with a finite set of cards (a shoe can consist of any number set of cards, typically 6 decks in casino games) and we can have some information of what cards have been played, we can estimate the probability of what the next card in the shoe will be and what the final dealer hand's will be so that we know what actions will give us the best chance at beating the dealer and maximize our winnings. As the game continues and we continue to draw more cards, we gain more information of what the shoe composition looks like and are able to predict with more certainty the likelihood of what cards will be drawn next. This is what Bayesian Probability is about, updating our beliefs as new information comes to light.

\subsubsection{Formulation}

Let:
\begin{itemize}
  \item \(H\): the player's current hand (a multiset of card ranks)
  \item \(U\): the dealer's upcard
  \item \(P\): the multiset of all cards dealt so far (including player's cards, dealer's cards, and previous rounds)
  \item \(R = \{\mathrm{A}, 2, 3, \dots, 10, \mathrm{J}, \mathrm{Q}, \mathrm{K}\}\): the set of card ranks
  \item \(m_r = 4\): the number of cards of rank \(r\) per deck, for each \(r \in R\)
  \item \(D \in \mathbb{N}\): the number of decks in the shoe
\end{itemize}

The initial count of cards of rank \(r\) in the shoe is
\[
  x_r^{(0)} = D \cdot m_r, \quad r\in R,
\]
and the initial total number of cards is
\[
  N^{(0)} = \sum_{r\in R} x_r^{(0)} = 52D.
\]

Let
\[
  O = H \cup \{U\} \cup P
\]
be the multiset of all observed cards. For each rank \(r\), define
\[
  \Delta_r = |\{c \in O : \mathrm{rank}(c)=r\}|,
\]
i.e., the number of cards of rank \(r\) removed from the shoe. Then the remaining count of rank \(r\) is
\[
  X_r = x_r^{(0)} - \Delta_r,
\]
and the total remaining cards is
\[
  N = \sum_{r\in R} X_r = 52D - |O|.
\]

\subsubsection{Probability of Drawing Rank Value}

Since draws are without replacement and the remaining cards are identically distributed, the probability that the next card has rank \(r\) given the current state \((H,U,P)\) is
\[
  \mathbb{P}(\mathrm{rank}(\text{next card}) = r \mid H, U, P)
  = \frac{X_r}{N}.
\]

\subsubsection{Proof}

By symmetry, each of the $N$ remaining cards is equally likely. Of these, exactly $X_r$ have rank $r$. 
Thus the probability of drawing rank $r$ is the fraction of remaining cards of that rank. 

\subsection{{Expected Value of Player's Hand}}
\label{sec:Expected Value}
For our model, we want to select the action that maximizes the player's expected value (AKA "expected utility").
In this case, expected value is the payoff we receive from our wager based on whether we choose to Hit, Stand, Double Down, or Split (if possible). 

\subsubsection{Formulation}
Let:
\begin{itemize}
    \item \( R = \{A, 2, 3, \dots, 10, J, Q, K\} \): Set of card ranks.
    \item \( X_r \): Number of cards of rank \(r\) remaining in the deck(s). 
    \item \( N = \sum_{r \in R} X_r \): Total number of cards remaining. 
    \item \( \Pr(r) = \dfrac{X_r}{N} \): Probability of drawing a card of rank \(r\). 
    \item \( H \): The player's current hand (a multiset).
    \item \( U \): The dealer's upcard.
    \item \( P \): The multiset of all cards drawn in previous rounds (observable history).
    \item \( v(H) \): The value of hand \(H\), treating Aces optimally.
    \item \( \sigma \in \{0,1,2,3\} \): Number of splits remaining.
    \item \( P_D(y \mid U, P) \): Probability that the dealer ends with value \(y\), given upcard \(U\) and card history \(P\).
\end{itemize}


\subsubsection{Payoff Function}
For simplicity denote \textit{x} as \( V(H_F)\) (the final value of the hand of the player) and \textit{y} denote the final hand value of the dealer,  
The payoff function is represented by
\[
g(x, y) =
\begin{cases}
+1, & x \leq 21 \text{ and } (y > 21 \text{ or } x > y) \\
0, & x \leq 21,\, y \leq 21,\, x = y \\
-1, & \text{otherwise}
\end{cases}
\]
\subsubsection{Expected Value Formulas}

\textbf{Expected Values for Each Action}

\begin{enumerate}
    \item \textbf{Standing}: Take no more cards
    \begin{equation}
        \mathrm{EV}_{\text{stand}}(H) = \sum_{y=17}^{22} P_D(y \mid U, P) \cdot g(v(H), y)
        \label{eq:EV_STAND}
    \end{equation}

    \item \textbf{Hitting}: Draw 1 card
    \begin{equation}
          \mathrm{EV}_{\text{hit}}(H, σ  ) = \sum_{r \in R} \frac{X_r}{N} \cdot \mathrm{EV}^*(H \cup \{r\}, σ  )
        \label{eq:EV_HIT}
    \end{equation}
You draw one card rank r) with probability \(x_r/N\).
Update your hand to \(H \cup \{r\}\). Choose the best action given the same number of splits remaining

    \item \textbf{Doubling Down}: Double the wager, draw once and stand
    \begin{equation}
        \mathrm{EV}_{\text{dd}}(H) = \sum_{r \in R} \frac{X_r}{N} \cdot 2 \cdot \mathrm{EV}_{\text{stand}}(H \cup \{r\})
        \label{eq:EV_DOUBLE}
    \end{equation}

\item \textbf{Splitting} (if $H = \{r, r\}$ and $\sigma \geq 1$)

  Let $X_{r_2}^{(-r_1)}$ denote the updated count of $r_2$ after removing $r_1$ from the deck.
  \begin{equation}
    \mathrm{EV}_{\text{split}}(H = \{r, r\}, \sigma ) = \sum_{r_1 \in R} \sum_{r_2 \in R} \frac{X_{r_1}}{N} \cdot \frac{X_{r_2}^{(-r_1)}}{N - 1} \cdot \left[ \mathrm{EV}^*(\{r, r_1\}, \sigma - 1) + \mathrm{EV}^*(\{r, r_2\}, \sigma - 1) \right]
    \label{eq:EV_SPLIT}
  \end{equation}
  $X_{r_2}^{(-r_1)}$ is the count of $r_2$ after removing the $r_1$ drawn from the first split.
  You draw 2 new cards, which couple with $r_1$ and $r_2$ separately to form 2 new hands: a right hand $\{r_1, r\}$ and left hand $\{r_2, r\}$.
  Each hand is played optimally with $\sigma - 1$ remaining splits. Since you now have 2 hands, their expected values are added together.
\end{enumerate}
\subsubsection{Optimal Action Value}

At every game state we choose the expected value that maximizes our return
\begin{equation}
\mathrm{EV}^*(H, σ  ) =
\max \left\{
\mathrm{EV}_{\text{stand}}(H),
\mathrm{EV}_{\text{hit}}(H, σ  ),
\mathrm{EV}_{\text{dd}}(H),
\left[\mathrm{EV}_{\text{split}}(H, σ  )\right]_{H = \{r, r\},\, σ   \geq 1}
\right\}
\label{eq:OBJ_FUNCTION}
    \end{equation}
The objective function defines a recursive dynamic program that can be solved backwards from terminal hands (bust or stand) upwards. 

\section{Method: Markov Decision Process }
A Markov Process refers to a process where the possibility of arriving at a certain state depends on the conditions of the current state. For example, the probability of us being able to split in the next state depends on whether or not we've exhausted all our possible splits in the current state.

However since the probability of the next state depends on the players decision and we want to maximize the probability obtaining the state with the highest expected value, we will be using a special case known as the Markov Decision Process (MDP) where outcomes are partly random and partly under the agents (e.g, the player) control

Let 
\begin{description}
  \item[States:] \( s = (H, U, X, \sigma) \), where:
  \begin{itemize}
    \item \( H \): Player's current hand (multiset of ranks)
    \item \( U \): Dealer's upcard
    \item \( X = (X_r)_{r \in R} \): Remaining counts of each rank
    \item \( \sigma \in \{0,1,2,3\} \): Number of splits remaining
  \end{itemize}
  \item[Actions:] \( A(s) = \{\mathrm{Stand},\; \mathrm{Hit},\; \mathrm{Double},\; \mathrm{Split}\ \text{if } H = \{r,r\} \text{ and } \sigma \geq 1 \}

The probability of reaching a particular next state $s'$ from $A(s)$ can be formulated by the following transition functions

\textbf{Transitions:}
  
  \begin{itemize}
    \item \textbf{Stand:} Dealer plays out.
    Transition to terminal state.\\
    \( P(s' \mid s, \mathrm{Stand}) = P_D(y \mid U, X) \), where \( s' \) encodes terminal outcome with dealer total \( y \)

    \item \textbf{Hit:} Draw one card \( r \) with probability \( \frac{X_r}{N} \).\\
    \( s' = (H \cup \{r\}, U, X^{(-r)}, σ) \)

    \item \textbf{Double:} Draw one card \( r \), then stand.
    Terminal transition.\\
    \( s' = \text{terminal with doubled reward, dealer plays out} \)

    \item \textbf{Split:} If allowed, draw two cards \( r_1, r_2 \) without replacement.\\
    Create two hands: \( (\{r, r_1\}, U, X^{(-\{r, r_1\})}, σ-1) \) and \( (\{r, r_2\}, U, X^{(-\{r, r_2\})}, σ-1) \)
  \end{itemize}

  \item[Rewards:] 0 for non-terminal steps.
  Terminal reward:
  \[
    R = g(\text{player total},\; \text{dealer total})
  \]
  Doubled if action = Double. 
  Intermediate transitions have reward 0. Terminal transitions deliver $ R = g($Player Total, Dealer Total) or doubled if action $a$ = Double Down
   
  \item[Discount:] \( \gamma = 1 \)
    Assume a risk factor of 1 since we care only about the total undiscounted payoff.\cite{sutton&bartol}
\end{description}

\subsection{Bellman Optimality Equations}

Our goal can therefore be written as maximizing the expected value of state S
\[
  V^*(s) = \max_{a \in A(s)} Q(s, a)
\]
where
\[
  Q(s,a) = \sum_{s'} P(s' \mid s, a) \left[ R(s,a,s') + V^*(s') \right]
\]

The expected value for each action can be given as:

\begin{itemize}
  \item \textbf{Stand:}
  \[
    Q(s, \mathrm{Stand}) = \sum_{y=17}^{22} P_D(y \mid U, X)\; g(v(H), y)
  \]

  \item \textbf{Hit:}
  \[
    Q(s, \mathrm{Hit}) = \sum_{r \in R} \frac{X_r}{N}\; V^*(H \cup \{r\}, U, X^{(-r)}, σ)
  \]

  \item \textbf{Double:}
  \[
    Q(s, \mathrm{Double}) = \sum_{r \in R} \frac{X_r}{N} \cdot 2 \sum_{y=17}^{22} P_D(y \mid U, X)\; g(v(H \cup \{r\}), y)
  \]
  \item \textbf{Split:}
  \begin{align*}
    Q(s, \mathrm{Split}) =
    \sum_{r_1, r_2 \in R} & \frac{X_{r_1}}{N} \cdot \frac{X^{(-r_1)}_{r_2}}{N - 1} \\
    & \times \left[
      V^*(\{r, r_1\}, U, X^{(-\{r, r_1\})}, σ-1)
      + V^*(\{r, r_2\}, U, X^{(-\{r, r_2\})}, σ-1)
    \right]
  \end{align*}
\end{itemize}

\subsection{Steps to Solve with Dynamic Programming}

\begin{enumerate}
  \item Initialize \( V_0(s) = 0 \) for all \( s \), or to known terminal values.
  \item Repeat until convergence:
  \[
    V_{k+1}(s) = \max_{a \in A(s)} \sum_{s'} P(s' \mid s, a) [ R(s,a,s') + V_k(s') ]
  \]
  \item Policy extraction:
  \[
    \pi^*(s) = \arg\max_{a \in A(s)} \sum_{s'} P(s' \mid s,a) [ R(s,a,s') + V^*(s') ]
  \]
\end{enumerate}

\subsection{Limitations}
This requires we pre-compute $P_d(y|U, x)$ for each dealer upcard $U$ and card-count $x$, iterate over all reachable states of $(H, U, x, σ)$ and store their values. So that in a given state we lookup the optimal policy $π$ to take.

However, this is gravely computationally intensive because there can be ${52D}\choose{k}$ possible states for a given hand of size K. For example, suppose a game with a shoe of 6 decks, the number of possible combinations for the players initial hand is ${312}\choose{2}$=$48,516$. This count skyrockets as you allow $k$ to grow as the player increases his hand count.
\documentclass[12pt]{article}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{enumitem}
\usepackage{geometry}
\geometry{margin=1in}

\section{Method: Monte Carlo Tree Search for Blackjack MDP}

Solving a Blackjack Markov Decision Process (MDP) optimally involves evaluating a vast number of game states and action sequences, which becomes computationally intractable under realistic card-counting assumptions. To approximate optimal play efficiently, we adopt a Monte Carlo Tree Search (MCTS) approach, which avoids exhaustive enumeration by simulating representative games and focusing computational effort on the most promising branches.

Monte Carlo methods have a long history in game analysis, originally popularized by Stanislaw Ulam to estimate the odds of winning solitaire through repeated simulation rather than exact computation. MCTS builds upon this philosophy by incrementally constructing a search tree guided by statistics collected through simulated rollouts.

In blackjack, MCTS approximates the action value function $Q(s, a)$ - the expected reward of taking action $a$ in state $s$ - by simulating the full game trajectories. This includes player decisions, as well as the dealer's play, conditioned on the current deck state. The dealer's terminal outcome distribution $P_d(y \mid U, x)$ is estimated dynamically using Monte Carlo rollouts: after each player-finalizing action (e.g., Stand or Double), the dealer's play is simulated from the current reduced shoe.

After sufficient simulations, the MCTS policy in each state converges toward an $\epsilon$-optimal policy, effectively approximating the optimal strategy without solving the full MDP analytically.

Here’s the basic loop MCTS runs over and over to build its search tree:

\begin{itemize}
    \item \textbf{Selection:} Starting from the root node (the current game state), the algorithm traverses the tree by selecting child nodes that optimize the Upper Confidence Bounds for Trees (UCT) criterion:
    \[
    a^* = \arg\max_a \left[ Q(s, a) + c \sqrt{\frac{\ln N(s)}{N(s, a)}} \right],
    \]
    where $N(s)$ is the visit count of state $s$, $N(s, a)$ is the visit count of action $a$ from state $s$, and $c = \sqrt{2}$ controls the exploration-exploitation tradeoff \cite{kocsis2006bandit}. Selection continues until a leaf node is reached.

    \item \textbf{Expansion:} If the selected leaf node is non-terminal, one or more child nodes are added to represent unexplored legal actions. The tree is allowed to grow to full game depth, which typically spans 4–6 steps depending on the sequence of player choices such as hits, splits, and doubles. No artificial depth limit is imposed.
\end{itemize} 

\begin{table}[H]
    \centering
    \begin{tabular}{lll}
        \hline
        \textbf{Player Hand} & \textbf{Dealer Upcard} & \textbf{Action} \\
        \hline
        \multicolumn{3}{l}{\textbf{Splitting Pairs}} \\
        Pair of 2s, 3s, 4s, 5s, 6s, 7s & 2-7 & Split \\
        Pair of 8s & Any & Split \\
        Pair of 9s & 3-6, 8-9 & Split \\
        Pair of Aces & Any & Split \\
        \hline
        \multicolumn{3}{l}{\textbf{Standing}} \\
        Hard 21 & Any & Stand \\
        Hard 17-20 & Any & Stand \\
        Hard 12-16 & 3-6 & Stand \\
        \hline
        \multicolumn{3}{l}{\textbf{Doubling Down}} \\
        Hard 9, 10, 11 & 2-9 & Double \\
        \hline
        \multicolumn{3}{l}{\textbf{Hitting}} \\
        Hard $\leq$ 11 & Any & Hit \\ 
        Hard 12-16 & 2, 7-Ace & Hit \\
        \hline
    \end{tabular}
    \caption{Blackjack Rollout Strategy}
    \label{tab:blackjack_strategy} % Label is correctly placed after caption
\end{table}

\begin{itemize} 
    \item \textbf{Backpropagation:} The terminal result is propagated back through all nodes visited during the current iteration. Each node updates its visit count and cumulative reward, refining the estimated value $Q(s, a)$ used in subsequent simulations.
\end{itemize}

We let MCTS run this loop many times. Once we're out of time or have run enough iterations, we look at the children of the very first node (our starting state) and pick the move that leads to the child with the best stats (usually the highest win rate or the most visited). This is the move MCTS recommends for our current situation in the Blackjack game.

Let
\begin{itemize}
    \item State \(s=(H, U, X, s_{\mathrm{split}})\), where:
    \item \begin{itemize}[leftmargin=1.5cm]
\item \(H\): Player hand (multiset of ranks)
\item \(U\): Dealer up\-card
\item \(X=(X_r)_{r\in R}\): Remaining card counts
\item \(s_{\mathrm{split}}\in\{0,1,2,3\}\): Splits remaining
    \item \end{itemize}
    \item Actions \(A(s)=\{\mathrm{Stand},\mathrm{Hit},\mathrm{Double}\}\) plus \(\mathrm{Split}\) if allowed.
\end{itemize}

\subsection{Node Statistics}
For each node \(s\) and action \(a\in A(s)\):
\begin{itemize}[leftmargin=1.5cm]
  \item \(N(s)\): visits of state \(s\)
  \item \(N(s,a)\): times action \(a\) taken
  \item \(W(s,a)\): total reward from those simulations
  \item \(Q(s,a)=W(s,a)/N(s,a)\): average action value
\end{itemize}

\subsection{{Deck Integration}}
Use exact draw probabilities:
\[\Pr(r)=\frac{X_r}{N},\quad X\to X^{(-r)},\quad N\to N-1.\]
Actions map to MDP transitions: Hit = sample one card, Stand = terminal + dealer simulation, Double = one draw then stand, Split = two subtrees.

\subsection{{Decision Rule}}
After \(M\) iterations,
\[
 \hat Q(s_0,a)=\frac{W(s_0,a)}{N(s_0,a)},\quad a^*=\arg\max_a \hat Q(s_0,a).
\]
This action approximates one that maximizes the true $Q^*(s_0,a$) in the full MDP

\subsection{{Limitations}}
\begin{itemize}[leftmargin=1.5cm]
  \item Memory grows with visited nodes (much smaller than full state space).  
  \item Time per iteration is tree depth + rollout length.  
  \item With \(M\to\infty\) and UCT, MCTS converges to optimal action.  
\end{itemize}

\section{Model Results}
We ran 25 independent Blackjack simulations with \(M=1000\) MCTS iterations per hand.  Figures~\ref{fig:bankroll}–\ref{fig:action_heatmaps} summarize:

\begin{figure}[H]
  \centering
  \includegraphics[width=.8\linewidth]{images/BankrollRoundsPerGame.png}
  \caption{Bankroll over 25 simulated games.}
  \label{fig:bankroll}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=.8\linewidth]{AvgBankroll.png}
  \caption{Average bankroll across all games.}
  \label{fig:avg_bankroll}
\end{figure}

\begin{figure}[H]
  \centering
  \subfigure[Hit]{\includegraphics[width=0.48\linewidth]{hit.png}}
  \subfigure[Stand]{\includegraphics[width=0.48\linewidth]{stand.png}}\\
  \subfigure[Double]{\includegraphics[width=0.48\linewidth]{double.png}}
  \subfigure[Split]{\includegraphics[width=0.48\linewidth]{split.png}}
  \caption{Heat-maps of action frequencies by player hand (y-axis) and dealer up-card (x-axis).}
  \label{fig:action_heatmaps}
\end{figure}



\section{Recommendations}
In this experiment, bet sizing is fixed and no surrender is allowed. Future work should let the agent vary its wager dynamically based on card counts and implement surrender when, given a certain risk appetite, the expected value is negative. Furthermore, the model should be evaluated against standard baselines such as basic strategy and Q-learning agents to contextualize its performance. Incorporating options like insurance and common side bets (e.g. Perfect Pairs, 21 + 3) would also make the simulation more comprehensive and aligned with real-world Blackjack scenarios, allowing the agent to learn richer policies under realistic casino conditions.

\section{Conclusion}

We presented a Monte Carlo Tree Search framework for solving a finite-deck Blackjack MDP, integrating Bayesian probability to model the evolving composition of the shoe. By maintaining and updating card draw probabilities after each action, the agent incorporates a form of Bayesian reasoning to condition future expectations on observed outcomes. This probabilistic foundation enables accurate dealer simulations, realistic player rollouts, and more informed action selection within the MCTS framework. Our results show that this approach can approximate near-optimal strategies without exhaustively evaluating the entire state space. Future work should extend this framework with dynamic bet sizing, surrender, insurance, side bets, and comparative evaluation against basic-strategy and Q-learning baselines to further assess its practical value in casino-style Blackjack.



\appendix


\section{Codebase}
\begingroup
\sloppy
Code and data are open source on GitHub:
\begin{itemize}[leftmargin=*]
  \item \url{https://github.com/ManuelLopezMe/Lets-Go-Gambling}  
  \item Data: \url{https://github.com/ManuelLopezMe/Lets-Go-Gambling/tree/main/data}
\end{itemize}
\endgroup

\section{Notation Summary}

\begin{table}[H]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{|c|p{10cm}|}
\hline
\textbf{Symbol} & \textbf{Description} \\
\hline
$s$ & Current game state (including player's hand, dealer upcard, and shoe composition) \\
$a$ & Action taken by the player (e.g., Hit, Stand, Double, Split) \\
$Q(s, a)$ & Estimated expected return (EV) of taking action $a$ in state $s$ \\
$c$ & UCT exploration constant, balancing exploration and exploitation ($c = \sqrt{2}$) \\
$N(s)$ & Number of times state $s$ has been visited \\
$N(s, a)$ & Number of times action $a$ has been taken from state $s$ \\
$U$ & Dealer's visible upcard \\
$x$ & Current composition of the shoe (card counts or probability vector) \\
$P_d(y \mid U, x)$ & Probability that the dealer ends with value $y$, given upcard $U$ and shoe $x$ \\
$N$ & Number of Monte Carlo rollouts per simulation (e.g., per dealer evaluation) \\
$\pi$ & Policy used to select actions during simulation (typically greedy or $\epsilon$-greedy) \\
\hline
\end{tabular}
\caption{Summary of notation used in the MCTS-based Blackjack model.}
\label{tab:notation}
\end{table}

\footnotesize
\bibliographystyle{unsrt}
\bibliography{bibliography_CIBB_file.bib} 
\normalsize
\end{document}
